{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "286cf841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# if current directory is not 'vqa-sdg', change it\n",
    "if os.path.basename(os.getcwd()) != 'vqa-sdg':\n",
    "    os.chdir('vqa-sdg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d8438e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import torchvision \n",
    "from torchvision.io import read_image \n",
    "from torchvision.utils import draw_bounding_boxes \n",
    "from time import time\n",
    "\n",
    "IMG_1 = read_image(\"./dataset/img/1072.jpg\")\n",
    "IMG_2 = read_image(\"./dataset/img/1159835.jpg\")\n",
    "IMG_3 = read_image(\"./dataset/img/1591896.jpg\")\n",
    "\n",
    "with open(\"./dataset/feat/sceneGraphs.json\") as json_file:\n",
    "    SCENE_GRAPH = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b1a26c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(long_path: str) -> str:\n",
    "    filename = os.path.basename(long_path)\n",
    "    raw_filename = os.path.splitext(filename)[0]\n",
    "\n",
    "    return filename, raw_filename\n",
    "\n",
    "\n",
    "\n",
    "def annotate_images(img_path, graph, num_obj=5, min_area_div=100): #TODO: Changing\n",
    "    COLORS = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"brown\", \"pink\", \"gray\", \"cyan\"]\n",
    "    img = torchvision.io.read_image(img_path)\n",
    "\n",
    "    img_area = img.size()[-1] * img.size()[-2]\n",
    "    annotated_imgs = []\n",
    "    bboxs = []\n",
    "\n",
    "    # Create num_obj images with only one annotation\n",
    "    for v in list(graph[\"objects\"].values()):\n",
    "        x, y, w, h = v[\"x\"], v[\"y\"], v[\"w\"], v[\"h\"]\n",
    "        if w * h * min_area_div < img_area:\n",
    "            continue\n",
    "        \n",
    "        bbox = [x, y, x + w, y + h]\n",
    "        bboxs.append(bbox)        \n",
    "        bbox = torch.tensor([bbox])\n",
    "\n",
    "        img_tensor = torchvision.utils.draw_bounding_boxes(\n",
    "            img, bbox, width=3, colors=[\"red\"]\n",
    "        )\n",
    "        img_pil = torchvision.transforms.ToPILImage()(img_tensor)\n",
    "\n",
    "        name = v[\"name\"]\n",
    "        annotated_imgs.append((name, img_pil))\n",
    "\n",
    "        num_obj -= 1\n",
    "        if num_obj == 0:\n",
    "            break\n",
    "    \n",
    "    # Draw all annotations on the image\n",
    "    complete_annot_img_tensor = torchvision.utils.draw_bounding_boxes(\n",
    "        img, torch.tensor(bboxs), width=3, colors=COLORS[:len(bboxs)]\n",
    "    )\n",
    "\n",
    "    return annotated_imgs, complete_annot_img_tensor\n",
    "\n",
    "\n",
    "\n",
    "def inference_hf(\n",
    "    model,\n",
    "    processor,\n",
    "    prompt,\n",
    "    api_key=None,\n",
    "    boilerplate_prompt=True,\n",
    "    img_path=None,\n",
    "    img_raw=None,\n",
    "    max_new_tokens=1500,\n",
    "    do_sample=False,\n",
    "    skip_special_tokens=True,\n",
    ") -> (str, float):\n",
    "    start_time = time()\n",
    "    if img_raw is None:\n",
    "        try:\n",
    "            img_raw = Image.open(img_path)\n",
    "        except Exception as e:\n",
    "            return str(e)\n",
    "\n",
    "    if boilerplate_prompt:\n",
    "        prompt = \"USER: <image>\\n\" + prompt + \"\\nASSISTANT:\"\n",
    "\n",
    "    inputs = processor(prompt, img_raw, return_tensors=\"pt\").to(0, torch.float16)\n",
    "    raw_output = model.generate(\n",
    "        **inputs, max_new_tokens=max_new_tokens, do_sample=do_sample\n",
    "    )\n",
    "    output = processor.decode(raw_output[0], skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "    if boilerplate_prompt:\n",
    "        output = output[output.index(\"ASSISTANT:\") + 11 :]\n",
    "\n",
    "    end_time = time()\n",
    "    seconds = end_time - start_time\n",
    "\n",
    "    return output + \"\\n\", seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cd2b2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change\n",
    "def nonvis_inference_runner(\n",
    "    model,\n",
    "    processor,\n",
    "    prompt_primary,\n",
    "    img_path,\n",
    "    scene_graph,\n",
    "    runner_config: dict,\n",
    "    prompt_inter=\"\",\n",
    "):\n",
    "    print(\"nonvis_inference_runner\", runner_config['pair_num'])\n",
    "    img_id_ext, img_id = get_filename(img_path)\n",
    "    raw_objs, complete_annot_tensor = annotate_images(\n",
    "        img_path, scene_graph[img_id], num_obj=runner_config[\"pair_num\"]\n",
    "    )\n",
    "\n",
    "    logging.info(f\"[{img_id_ext}] - Nonvis Inference started...\")\n",
    "\n",
    "    outs = []\n",
    "    total_sec = 0\n",
    "\n",
    "    for i, obj in enumerate(raw_objs):\n",
    "        out, sec = inference_hf(\n",
    "            model,\n",
    "            processor,\n",
    "            prompt_primary.format(number=i + 1, name=obj[0]),\n",
    "            img_raw=obj[1],\n",
    "        )\n",
    "        outs.append(out)\n",
    "        total_sec += sec\n",
    "\n",
    "    logging.info(f\"[{img_id_ext}] - Nonvis Inference finished ({sec}s)\")\n",
    "\n",
    "    return \"\\n\".join(outs), total_sec, \"\", 0, {\"id\": img_id ,\"complete_tensor\": complete_annot_tensor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bfac93de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import LlavaForConditionalGeneration, VipLlavaForConditionalGeneration, AutoModel, AutoProcessor, AutoModelForCausalLM, AutoModelForPreTraining\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Set the seed for random number generators for reproducibility.\n",
    "\n",
    "    Args:\n",
    "    - seed (int): The seed value.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def load_model(model_path, model_family, low_cpu_mem_usage, device = \"cuda\", seed = 42):\n",
    "    \n",
    "    MODEL_LOADER_DICT = {\n",
    "        \"llava\" : LlavaForConditionalGeneration,\n",
    "        \"vip_llava\": VipLlavaForConditionalGeneration,\n",
    "        \"auto\": AutoModel,\n",
    "        \"llama\": AutoModelForCausalLM,\n",
    "        \"llava-1.6\": AutoModelForPreTraining\n",
    "    }\n",
    "    \n",
    "    model, processor = None, None\n",
    "    if \"openai\" not in model_family:\n",
    "        set_seed(seed)\n",
    "        model = MODEL_LOADER_DICT[model_family].from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype = torch.float16,\n",
    "            low_cpu_mem_usage = low_cpu_mem_usage\n",
    "        ).to(device)\n",
    "        processor = AutoProcessor.from_pretrained(model_path)\n",
    "        \n",
    "    print(f\"Loaded {model_path}\")\n",
    "    \n",
    "    return model, processor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c90d9d",
   "metadata": {},
   "source": [
    "# Model, Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ffdc56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.11it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded llava-hf/vip-llava-13b-hf\n"
     ]
    }
   ],
   "source": [
    "model, processor = load_model(\"llava-hf/vip-llava-13b-hf\", \"vip_llava\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec5559e",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "14f1d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"\"\"\n",
    "Generate questions that involves complex reasoning with an equal mix of 'WHAT,' 'WHERE', and 'WHO'. For each question, generate a short factoid answer and long answer that contains the reason of short answer pick.\n",
    "    \n",
    "What are {number} possible questions-answers about the image?\n",
    "\n",
    "Present the question-answer pairs in format:\n",
    "1. <QUESTION-1>\n",
    "S. <SHORT-ANSWER-1>\n",
    "L. <LONG-ANSWER-1>\n",
    "<<line-break>>\n",
    "\"\"\"\n",
    "\n",
    "vit_prompt = \"\"\"\n",
    "Generate {number} questions that involves complex reasoning using about {name} object\n",
    "in the {color} bounding box. \n",
    "\n",
    "Present the question-answer-reasons in format:\n",
    "1. <QUESTION>\n",
    "S. <SHORT-ANSWER>\n",
    "L. <LONG-ANSWER>\n",
    "\"\"\"\n",
    "\n",
    "vit_base_prompt = \"\"\"\n",
    "Generate a question that involves complex reasoning using about {name} object in the bounding box. \n",
    "For each question, generate a short factoid answer and long answer that contains the reason of short answer pick.\n",
    "\n",
    "Present the question-answer-reasons in format:\n",
    "{number}. <your-question>\n",
    "S. <your-short-answer>\n",
    "L. <your-long-answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19451d32",
   "metadata": {},
   "source": [
    "# Image, Graph, Runner Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cc59fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_ID = 1159835\n",
    "IMG_PATH = f\"./dataset/img/{IMG_ID}.jpg\"\n",
    "\n",
    "with open(\"./dataset/feat/sceneGraphs.json\") as json_file:\n",
    "    SCENE_GRAPH = json.load(json_file)\n",
    "\n",
    "runner_config = {\n",
    "    \"pair_num\": 3,\n",
    "    \"is_multistep\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3656dc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonvis_inference_runner 3\n"
     ]
    }
   ],
   "source": [
    "inference_runner = nonvis_inference_runner\n",
    "\n",
    "primary_out, primary_sec, inter_out, inter_sec, meta = inference_runner(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    prompt_primary=vit_base_prompt,\n",
    "    prompt_inter=\"\",\n",
    "    img_path=IMG_PATH,\n",
    "    scene_graph=SCENE_GRAPH,\n",
    "    runner_config=runner_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "15251669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_annotated_img(tensor, path):\n",
    "    img_pil = torchvision.transforms.ToPILImage()(tensor)\n",
    "    img_pil.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f4c25334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:27<00:00,  3.94s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "out = \"\"\n",
    "\n",
    "COL = [i for i in range(len(names))]\n",
    "\n",
    "for obj_i in tqdm(COL):\n",
    "    vit_out = inference_hf(\n",
    "        model, processor, \n",
    "        vit_prompt.format(\n",
    "            number = 1,\n",
    "            name = names[obj_i], \n",
    "            color = colors[obj_i]\n",
    "        ),\n",
    "        img_raw = img\n",
    "    )\n",
    "    out += vit_out + \"------------------------------------\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "31f2a6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is the purpose of the door within red bounding box?\n",
      "S. To provide access to the outside.\n",
      "L. The door within red bounding box is a traditional wooden door that likely leads to a porch or deck, providing a means for people to enter and exit the house.\n",
      "------------------------------------\n",
      "1. What is the significance of the ornament within the orange bounding box on the Christmas tree within the red rectangle?\n",
      "S. It is a decoration.\n",
      "L. The ornament within the orange bounding box is a decorative item that is commonly used to adorn Christmas trees within the red rectangle during the holiday season. It adds a festive touch to the tree and is often chosen for its color, shape, or theme.\n",
      "------------------------------------\n",
      "1. What is the name of the movie playing on the television within blue bounding box?\n",
      "S. Star Wars\n",
      "L. The movie playing on the television within blue bounding box is Star Wars, as indicated by the visible logo and the spacecraft visible on the screen.\n",
      "------------------------------------\n",
      "1. What is the significance of the ornament within the green bounding box on the Christmas tree within the red rectangle?\n",
      "S. It is a decoration.\n",
      "L. The ornament within the green bounding box is a decorative item used to enhance the festive appearance of the Christmas tree within the red rectangle, which is a traditional practice during the holiday season.\n",
      "------------------------------------\n",
      "1. <QUESTION>\n",
      "What is the significance of the ornament within cyan bounding box in the context of the Christmas tree within red rectangle?\n",
      "S. <SHORT-ANSWER>\n",
      "The ornament within cyan bounding box is a decorative item that adds to the festive atmosphere of the Christmas tree within red rectangle.\n",
      "L. <LONG-ANSWER>\n",
      "The ornament within cyan bounding box is a common decoration for Christmas trees, symbolizing joy and celebration. It is often used in conjunction with other ornaments to create a visually appealing and festive display.\n",
      "------------------------------------\n",
      "1. <QUESTION>\n",
      "What is the significance of the ornament object in pink bounding box in the context of the Christmas tree within red rectangle?\n",
      "\n",
      "S. <SHORT-ANSWER>\n",
      "The ornament object in pink bounding box is a decoration that adds to the festive atmosphere of the Christmas tree within red rectangle.\n",
      "\n",
      "L. <LONG-ANSWER>\n",
      "The ornament object in pink bounding box is a decorative item that is commonly used to adorn Christmas trees within red rectangle. It is often chosen for its bright color and association with the holiday season. The ornament object in pink bounding box, along with other decorations like the one within yellow rectangle, contributes to the overall aesthetic of the Christmas tree within red rectangle, making it more visually appealing and festive.\n",
      "------------------------------------\n",
      "1. What is the significance of the red poinsettias in the yellow bounding box on the Christmas tree within the red rectangle?\n",
      "\n",
      "S. They are a traditional decoration for Christmas.\n",
      "L. The red poinsettias in the yellow bounding box are often used as a festive decoration during the Christmas season. They are native to Mexico and have been associated with Christmas since the 16th century. They are believed to bring good luck and are commonly used to adorn homes and churches during the holiday season.\n",
      "------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf552840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Patrick (Mistral)",
   "language": "python",
   "name": "mistral_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
