{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c9e738e",
   "metadata": {},
   "source": [
    "# BakLLaVa (Mistral-7B) Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37e0b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def load_config(config_path,config_name):\n",
    "    with open(os.path.join(config_path, config_name)) as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "config = load_config(\"../\",\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfdfbf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants & Seed\n",
    "SEED = config[\"seed\"]\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Inputs\n",
    "IMG_PATH = config[\"image_path\"]\n",
    "IMG_ID = IMG_PATH.split('/')[-1].split('.')[0]\n",
    "\n",
    "# Prompt\n",
    "LR_PROMPT_TYPE = config['prompt']['lr']['filename']\n",
    "QG_PROMPT_TYPE = config[\"prompt\"][\"qg\"][\"filename\"]\n",
    "LR_PROMPT_PATH = f\"{config['prompt']['lr']['parent']}{LR_PROMPT_TYPE}\"\n",
    "QG_PROMPT_PATH = f'{config[\"prompt\"][\"qg\"][\"parent\"]}{QG_PROMPT_TYPE}'\n",
    "with open(LR_PROMPT_PATH, \"r\") as file:\n",
    "    LR_PROMPT= file.read()\n",
    "with open(QG_PROMPT_PATH,\"r\") as file:\n",
    "    QG_PROMPT = file.read()\n",
    "\n",
    "# Params\n",
    "MODEL_PATH = config[\"bakllava\"][\"model_path\"]\n",
    "PAIR_NUM = config[\"bakllava\"][\"params\"][\"pair_count\"]\n",
    "\n",
    "# Result\n",
    "LR_RESULT_PARENT_PATH = config[\"result\"][\"bakllava\"][\"lr_path\"]\n",
    "QG_RESULT_PARENT_PATH = config[\"result\"][\"bakllava\"][\"qg_path\"]\n",
    "JSON_PARENT_PATH = config[\"result\"][\"bakllava\"][\"json_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d421d26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mistral_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.94it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_PATH, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    "#     load_in_4bit=True\n",
    ").to(0)\n",
    "processor = AutoProcessor.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecbc3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prompt = f\"USER: <image>\\n{LR_PROMPT.format(number = 5)}\\nASSISTANT:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3621ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "raw_image = Image.open(IMG_PATH)\n",
    "\n",
    "# lr_prompt = f\"USER: <image>\\n{LR_PROMPT.format(number = 5)}\\nASSISTANT:\"\n",
    "# lr_inputs = processor(lr_prompt, raw_image, return_tensors='pt') \\\n",
    "#             .to(0, torch.float16)\n",
    "# lr_output_raw = model.generate(**lr_inputs, max_new_tokens=200, do_sample=False)\n",
    "# lr_output = processor.decode(lr_output_raw[0], skip_special_tokens=True)\n",
    "# lr_output_trunc = lr_output[lr_output.index(\"ASSISTANT:\") + 11:]\n",
    "\n",
    "\n",
    "# qg_prompt = f\"USER: <image>\\n{QG_PROMPT.format(desc = lr_output_trunc, number = PAIR_NUM)}\\nASSISTANT:\"\n",
    "# qg_inputs = processor(qg_prompt, raw_image, return_tensors='pt') \\\n",
    "#             .to(0, torch.float16)\n",
    "# qg_output_raw = model.generate(**qg_inputs, max_new_tokens=200, do_sample=False)\n",
    "# qg_output = processor.decode(qg_output_raw[0], skip_special_tokens=True)\n",
    "# qg_output_trunc = qg_output[qg_output.index(\"ASSISTANT:\") + 11:]\n",
    "\n",
    "\n",
    "story_prompt = f\"USER: <image>\\nGenerate a 400 words description about the image. AVOID using any external assumptions or informations!\\nASSISTANT:\"\n",
    "story_inputs = processor(story_prompt, raw_image, return_tensors='pt') \\\n",
    "            .to(0, torch.float16)\n",
    "story_output_raw = model.generate(**story_inputs, max_new_tokens = 500, do_sample=False)\n",
    "story_output = processor.decode(story_output_raw[0], skip_special_tokens=True)\n",
    "story_output_trunc = story_output[story_output.index(\"ASSISTANT:\") + 11:]\n",
    "\n",
    "\n",
    "\n",
    "str8_prompt = f\"USER: <image>\\nWhat are 5 possible questions-answers about the image?\\nASSISTANT:\"\n",
    "str8_inputs = processor(str8_prompt, raw_image, return_tensors='pt') \\\n",
    "            .to(0, torch.float16)\n",
    "str8_output_raw = model.generate(**str8_inputs, max_new_tokens = 500, do_sample=False)\n",
    "str8_output = processor.decode(str8_output_raw[0], skip_special_tokens=True)\n",
    "str8_output_trunc = str8_output[str8_output.index(\"ASSISTANT:\") + 11:]\n",
    "\n",
    "\n",
    "qgstory_prompt = f\"USER: <image>\\n<PASSAGE>\\n{story_output_trunc}\\n\\n\\nBased on the <PASSAGE>, what are 5 possible questions & answers?\\nASSISTANT:\"\n",
    "qgstory_inputs = processor(qgstory_prompt, raw_image, return_tensors='pt') \\\n",
    "            .to(0, torch.float16)\n",
    "qgstory_output_raw = model.generate(**qgstory_inputs, max_new_tokens = 500, do_sample=False)\n",
    "qgstory_output = processor.decode(qgstory_output_raw[0], skip_special_tokens=True)\n",
    "qgstory_output_trunc = qgstory_output[qgstory_output.index(\"ASSISTANT:\") + 11:]\n",
    "\n",
    "\n",
    "timestamp = datetime.now().strftime('%d%m%H%M')\n",
    "LR_RESULT_FILENAME = f\"{IMG_ID}_{datetime.now().strftime('%d%m%H%M')}.txt\" \n",
    "QG_RESULT_FILENAME = f\"{IMG_ID}_{datetime.now().strftime('%d%m%H%M')}.txt\" \n",
    "LR_RESULT_PATH = LR_RESULT_PARENT_PATH + LR_RESULT_FILENAME\n",
    "QG_RESULT_PATH = QG_RESULT_PARENT_PATH + QG_RESULT_FILENAME\n",
    "\n",
    "with open(LR_RESULT_PATH,\"w\") as f:\n",
    "    f.write(lr_output_trunc)\n",
    "with open(QG_RESULT_PATH,\"w\") as f:\n",
    "    f.write(qg_output_trunc)\n",
    "with open(f\"../result/inference/BakLLaVa/story/{IMG_ID}_{timestamp}_story.txt\",\"w\") as f:\n",
    "    f.write(story_output_trunc)\n",
    "with open(f\"../result/inference/BakLLaVa/qgstory/{IMG_ID}_{timestamp}_qgstory.txt\",\"w\") as f:\n",
    "    f.write(qgstory_output_trunc)\n",
    "with open(f\"../result/inference/BakLLaVa/str8/{IMG_ID}_{timestamp}_str8.txt\",\"w\") as f:\n",
    "    f.write(str8_output_trunc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Patrick (Mistral)",
   "language": "python",
   "name": "mistral_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
