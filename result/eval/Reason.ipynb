{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40fea938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ············································\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import os\n",
    "import openai\n",
    "import time\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from getpass import getpass\n",
    "\n",
    "OPENAI_API_KEY = getpass()\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43737db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Config Settings\n",
    "def load_config(config_path,config_name):\n",
    "    with open(os.path.join(config_path, config_name)) as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "config = load_config(\"../../\",\"config.yaml\")\n",
    "\n",
    "\n",
    "# -- Prompt Path to be read\n",
    "EVAL_PROMPT_PARENT_PATH = config[\"prompt\"][\"eval\"][\"parent\"]\n",
    "FACTOID_PROMPT_PATH = EVAL_PROMPT_PARENT_PATH + config[\"prompt\"][\"eval\"][\"factoid\"]\n",
    "REASONING_PROMPT_PATH = EVAL_PROMPT_PARENT_PATH + config[\"prompt\"][\"eval\"][\"reasoning\"]\n",
    "COMPLETE_PROMPT_PATH = EVAL_PROMPT_PARENT_PATH + config[\"prompt\"][\"eval\"][\"complete\"]\n",
    "\n",
    "\n",
    "# -- Prompt Reading\n",
    "with open(FACTOID_PROMPT_PATH) as file:\n",
    "    FACTOID_PROMPT_TEMPLATE = file.read()\n",
    "with open(REASONING_PROMPT_PATH) as file:\n",
    "    REASONING_PROMPT_TEMPLATE = file.read()\n",
    "with open(COMPLETE_PROMPT_PATH) as file:\n",
    "    COMPLETE_PROMPT_TEMPLATE = file.read()\n",
    "    \n",
    "# -- Result Path to be evaluated\n",
    "JSON_PARENT= config[\"eval\"][\"sample\"][\"json_parent\"]\n",
    "JSON_FILENAME = config[\"eval\"][\"sample\"][\"json_filename\"]\n",
    "JSON_PATH = JSON_PARENT + JSON_FILENAME\n",
    "EVAL_ID = JSON_FILENAME.split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "217a346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_json(json_file_path):\n",
    "    try:\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{json_file_path}' not found.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON in '{json_file_path}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        \n",
    "unpacked_results = unpack_json(JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b96d864e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1.0,\n",
       " 'subject_id': 1.0,\n",
       " 'img_id': 'sample_001',\n",
       " 'time': 538.0,\n",
       " 'question': 'What time does the photo is taken?',\n",
       " 'short_answer': 'Approximately at afternoon around 1 PM',\n",
       " 'reasoned_answer': 'Because it is very sunny and there is no existence of cloud'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpacked_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "803879cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'curr_desc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 57\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     curr_desc \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     55\u001b[0m sa_res \u001b[38;5;241m=\u001b[39m chat(\n\u001b[1;32m     56\u001b[0m     fact_prompt\u001b[38;5;241m.\u001b[39mformat_prompt(\n\u001b[0;32m---> 57\u001b[0m         description \u001b[38;5;241m=\u001b[39m \u001b[43mcurr_desc\u001b[49m,\n\u001b[1;32m     58\u001b[0m         question \u001b[38;5;241m=\u001b[39m curr_q,\n\u001b[1;32m     59\u001b[0m         answer \u001b[38;5;241m=\u001b[39m curr_sa\n\u001b[1;32m     60\u001b[0m     )\u001b[38;5;241m.\u001b[39mto_messages()\n\u001b[1;32m     61\u001b[0m )\n\u001b[1;32m     63\u001b[0m ra_res \u001b[38;5;241m=\u001b[39m chat(\n\u001b[1;32m     64\u001b[0m     reason_prompt\u001b[38;5;241m.\u001b[39mformat_prompt(\n\u001b[1;32m     65\u001b[0m         description \u001b[38;5;241m=\u001b[39m curr_desc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m     )\u001b[38;5;241m.\u001b[39mto_messages()\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     71\u001b[0m ids\u001b[38;5;241m.\u001b[39mappend(curr_id)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'curr_desc' is not defined"
     ]
    }
   ],
   "source": [
    "# -- Init\n",
    "ids, factoid_scores, reason_scores = [],[],[]\n",
    "if \"sample\" in EVAL_ID:\n",
    "    subject_ids, img_ids, times = [],[],[]\n",
    "raw_scores = []\n",
    "\n",
    "\n",
    "# -- LLM & Prompt Prep\n",
    "chat = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "fact_sys_template = (\n",
    "    \"You are a helpful evaluator that is responsible to grade answer of a question based on the description\"\n",
    ")\n",
    "fact_sys_prompt = SystemMessagePromptTemplate.from_template(fact_sys_template)\n",
    "fact_human_prompt = HumanMessagePromptTemplate.from_template(FACTOID_PROMPT_TEMPLATE)\n",
    "fact_prompt = ChatPromptTemplate.from_messages(\n",
    "    [fact_sys_prompt, fact_human_prompt]\n",
    ")\n",
    "\n",
    "reason_sys_template = (\n",
    "    \"You are a helpful evaluator that is responsible to grade answer of a question based on the description\"\n",
    ")\n",
    "reason_sys_prompt = SystemMessagePromptTemplate.from_template(reason_sys_template)\n",
    "reason_human_prompt = HumanMessagePromptTemplate.from_template(REASONING_PROMPT_TEMPLATE)\n",
    "reason_prompt = ChatPromptTemplate.from_messages(\n",
    "    [reason_sys_prompt, reason_human_prompt]\n",
    ")\n",
    "\n",
    "# sys_template = (\n",
    "#     \"You are an NLP expert that is responsible to grade answer & reasoning of a question based on the description\"\n",
    "# )\n",
    "# sys_prompt = SystemMessagePromptTemplate.from_template(sys_template)\n",
    "# msg_prompt = HumanMessagePromptTemplate.from_template(COMPLETE_PROMPT_TEMPLATE)\n",
    "# complete_prompt = ChatPromptTemplate.from_messages(\n",
    "#     [sys_prompt, msg_prompt]\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# -- Running Evaluation Callback\n",
    "with get_openai_callback() as cb:\n",
    "    for sample in tqdm(unpacked_results):\n",
    "        curr_id = sample[\"id\"]\n",
    "        curr_q = sample[\"question\"]\n",
    "        curr_sa = sample[\"short_answer\"]\n",
    "        curr_ra = sample[\"reasoned_answer\"]\n",
    "        \n",
    "        # Differentiate Sample surveys vs. LMM Eval\n",
    "        if \"sample\" in EVAL_ID: \n",
    "            curr_subject_id = sample[\"subject_id\"]\n",
    "            curr_img_id = sample[\"img_id\"]\n",
    "            curr_time = sample[\"time\"]\n",
    "        else:\n",
    "            curr_desc = sample[\"description\"]\n",
    "        \n",
    "#         sa_res = chat(\n",
    "#             fact_prompt.format_prompt(\n",
    "#                 description = curr_desc,\n",
    "#                 question = curr_q,\n",
    "#                 answer = curr_sa\n",
    "#             ).to_messages()\n",
    "#         )\n",
    "\n",
    "#         ra_res = chat(\n",
    "#             reason_prompt.format_prompt(\n",
    "#                 description = curr_desc,\n",
    "#                 question = curr_q,\n",
    "#                 reason = curr_ra\n",
    "#             ).to_messages()\n",
    "#         )\n",
    "        \n",
    "        sa_res = chat(\n",
    "            fact_prompt.format_prompt(\n",
    "                description = curr_desc,\n",
    "                question = curr_q,\n",
    "                answer = curr_sa\n",
    "            ).to_messages()\n",
    "        )\n",
    "        \n",
    "        ra_res = chat(\n",
    "            reason_prompt.format_prompt(\n",
    "                description = curr_desc,\n",
    "                question = curr_q,\n",
    "                reason = curr_ra\n",
    "            ).to_messages()\n",
    "        )\n",
    "\n",
    "        ids.append(curr_id)\n",
    "        if \"sample\" in EVAL_ID:\n",
    "            subject_ids.append(curr_subject_id)\n",
    "            img_ids.append(curr_img_id)\n",
    "            times.append(curr_time)\n",
    "        factoid_scores.append(sa_res.content)\n",
    "        reason_scores.append(ra_res.content)\n",
    "    \n",
    "#     with open(f\"./bill/{EVAL_ID}.txt\", \"w\") as price_log_file:\n",
    "#         price_log_file.write(str(cb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d138a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./bill/{EVAL_ID}.txt\", \"w\") as price_log_file:\n",
    "    price_log_file.write(str(dt.now())+\"\\n\\n\")\n",
    "    price_log_file.write(str(cb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6a043fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs,logics,clears,details,irrels,plauss=[],[],[],[],[],[]\n",
    "\n",
    "for ID, fs, rs in zip(ids,factoid_scores, reason_scores):\n",
    "    logic, clear, detail, irrel, plaus = (int(s) for s in rs.split(\";\"))\n",
    "    accs.append(int(fs))\n",
    "    logics.append(logic)\n",
    "    clears.append(clear)\n",
    "    details.append(detail)\n",
    "    irrels.append(irrel)\n",
    "    plauss.append(plaus)\n",
    "    \n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"id\" : i,\n",
    "        \"accuracy\": a,\n",
    "        \"logic\" : l,\n",
    "        \"clarity\" : c,\n",
    "        \"detail\" : d,\n",
    "        \"irrelevance\" : ir,\n",
    "        \"plausibility\" : p\n",
    "    }\n",
    "    for i,a,l,c,d,ir,p in zip(ids,accs,logics,clears,details,irrels,plauss)\n",
    "]\n",
    "\n",
    "if \"sample\" in EVAL_ID:\n",
    "    data = [\n",
    "        {\n",
    "            \"id\" : i,\n",
    "            \"subject_id\" : si,\n",
    "            \"image_id\" : ii,\n",
    "            \"time\" : t,\n",
    "            \"accuracy\": a,\n",
    "            \"logic\" : l,\n",
    "            \"clarity\" : c,\n",
    "            \"detail\" : d,\n",
    "            \"irrelevance\" : ir,\n",
    "            \"plausibility\" : p\n",
    "        }\n",
    "        for i,si,ii,t,a,l,c,d,ir,p in zip(ids, subject_ids, img_ids, times,accs,logics,clears,details,irrels,plauss)\n",
    "    ]\n",
    "\n",
    "with open(f\"{EVAL_id}.json\", 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c643dc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Patrick (Eval)",
   "language": "python",
   "name": "eval_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
