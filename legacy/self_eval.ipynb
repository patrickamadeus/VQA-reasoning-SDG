{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094ae40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mistral_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def load_config(config_path,config_name):\n",
    "    with open(os.path.join(config_path, config_name)) as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "config = load_config(\"../\",\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7dbff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = config[\"eval\"][\"result_model_name\"]\n",
    "IMG_DATASET = config[\"image_dataset\"]\n",
    "IMG_DIR = config[\"eval\"][\"img_dir\"].format(image_dataset = IMG_DATASET)\n",
    "RESULT_JSON = config[\"eval\"][\"result_json\"]\n",
    "RESULT_PATH = config[\"eval\"][\"result_dir\"].format(\n",
    "                                            model_name = MODEL_NAME,\n",
    "                                            result_json = RESULT_JSON\n",
    "                                          )\n",
    "\n",
    "EVALUATOR_MODEL_PATH = config[\"eval\"][\"evaluator_model_path\"]\n",
    "\n",
    "## TO BE DECIDED\n",
    "FACTOID_PROMPT_PATH = config[\"eval\"][\"factoid_prompt\"]\n",
    "REASONING_PROMPT_PATH = config[\"eval\"][\"reasoning_prompt\"]\n",
    "R0_PATH = config[\"eval\"][\"r0\"]\n",
    "R1_PATH = config[\"eval\"][\"r1\"]\n",
    "R2_PATH = config[\"eval\"][\"r2\"]\n",
    "R3_PATH = config[\"eval\"][\"r3\"]\n",
    "R4_PATH = config[\"eval\"][\"r4\"]\n",
    "\n",
    "with open(FACTOID_PROMPT_PATH,\"r\") as file:\n",
    "    FACTOID_EVAL_PROMPT = file.read()\n",
    "with open(REASONING_PROMPT_PATH,\"r\") as file:\n",
    "    REASONING_EVAL_PROMPT = file.read()\n",
    "\n",
    "with open(R0_PATH,\"r\") as file:\n",
    "    R0_PROMPT = file.read()\n",
    "with open(R1_PATH,\"r\") as file:\n",
    "    R1_PROMPT = file.read()\n",
    "with open(R2_PATH,\"r\") as file:\n",
    "    R2_PROMPT = file.read()\n",
    "with open(R3_PATH,\"r\") as file:\n",
    "    R3_PROMPT = file.read()\n",
    "with open(R4_PATH,\"r\") as file:\n",
    "    R4_PROMPT = file.read()\n",
    "#####\n",
    "\n",
    "EVAL_RESULT_PATH = config[\"eval\"][\"eval_result_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a24a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_json(json_file_path):\n",
    "    try:\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{json_file_path}' not found.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON in '{json_file_path}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "import json\n",
    "RESULT_PATH = \"sdg_out.json\"\n",
    "res_json = unpack_json(RESULT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f7b573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_llava(model, processor, prompt, img, max_new_tokens=500, do_sample=False, skip_special_tokens=True) -> str:\n",
    "    complete_prompt = f\"USER: <image>\\n{prompt}\\nASSISTANT:\"\n",
    "    \n",
    "    inputs = processor(\n",
    "        complete_prompt, \n",
    "        img, \n",
    "        return_tensors = 'pt'\n",
    "    ).to(0, torch.float16)\n",
    "    \n",
    "    raw_output = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens = max_new_tokens, \n",
    "        do_sample = do_sample\n",
    "    )\n",
    "    \n",
    "    output = processor.decode(raw_output[0], skip_special_tokens = skip_special_tokens)\n",
    "    output_trunc = output[output.index(\"ASSISTANT:\") + 11:]\n",
    "    \n",
    "    return output_trunc\n",
    "\n",
    "def exec_time(to, tt) -> str:\n",
    "    time_difference = tt - to\n",
    "\n",
    "    hours, remainder = divmod(time_difference.seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "    result_format = f\"{hours}h{minutes}m{seconds}s\"\n",
    "    \n",
    "    return result_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d5bc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.01it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    EVALUATOR_MODEL_PATH, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    "#     load_in_4bit=True\n",
    ").to(0)\n",
    "processor = AutoProcessor.from_pretrained(EVALUATOR_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "74f5ba34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:14<02:09,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid image sample_008 file\n",
      "Invalid image sample_008 file\n",
      "Invalid image sample_008 file\n",
      "Invalid image sample_008 file\n",
      "Invalid image sample_008 file\n",
      "Invalid image sample_008 file\n",
      "Invalid image sample_008 file\n",
      "Invalid image sample_008 file\n",
      "Invalid image sample_008 file\n",
      "Invalid image sample_008 file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:12<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "ids, factoid_scores, reasoning_scores = [], [], []\n",
    "subject_ids, img_ids, times = [],[],[]\n",
    "\n",
    "for res in tqdm(res_json):\n",
    "    # -- Unpacking json\n",
    "    res_id = res[\"id\"]\n",
    "    img_id = res[\"img_id\"]\n",
    "    subject_id = res[\"subject_id\"]\n",
    "    time = res[\"time\"]\n",
    "    q = res[\"question\"]\n",
    "    sa = res[\"short_answer\"]\n",
    "    ra = res[\"reasoned_answer\"]\n",
    "    \n",
    "    # -- Img validation\n",
    "    IMG_PATH = os.path.join(IMG_DIR, f\"{img_id}.jpg\")\n",
    "    \n",
    "    if os.path.isfile(IMG_PATH):\n",
    "        raw_image = Image.open(IMG_PATH)\n",
    "    else:\n",
    "        print(f\"Invalid image {img_id} file\")\n",
    "        continue\n",
    "#     print()\n",
    "#     print(FACTOID_EVAL_PROMPT.format(question = q, answer = sa))\n",
    "    \n",
    "    # -- Inference process\n",
    "    raw_factoid_eval_res = inference_llava(\n",
    "        model, processor,\n",
    "        FACTOID_EVAL_PROMPT.format(question = q, answer = sa),\n",
    "        raw_image\n",
    "    )\n",
    "    \n",
    "#     raw_reasoning_eval_res = inference_llava(\n",
    "#         model, processor,\n",
    "#         REASONING_EVAL_PROMPT.format(question = q, answer = sa, reason = ra),\n",
    "#         raw_image\n",
    "#     )\n",
    "    Rs = [R0_PROMPT, R1_PROMPT, R2_PROMPT, R3_PROMPT, R4_PROMPT]\n",
    "    scores = []\n",
    "    for R in Rs:\n",
    "        r_eval_res = inference_llava(\n",
    "            model, processor,\n",
    "            R.format(question = q, answer = sa, reason = ra),\n",
    "            raw_image\n",
    "        )\n",
    "        scores.append(r_eval_res)\n",
    "    \n",
    "    # -- Log result\n",
    "    ids.append(res_id)\n",
    "    if \"sample\" in img_id:\n",
    "        subject_ids.append(subject_id)\n",
    "        img_ids.append(img_id)\n",
    "        times.append(time)\n",
    "    factoid_scores.append(raw_factoid_eval_res)\n",
    "    reasoning_scores.append(';'.join(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cdbd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "### this is for sdg_out, please fix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2beff78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [01:58<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "ids, factoid_scores, reasoning_scores = [], [], []\n",
    "\n",
    "for res in tqdm(res_json):\n",
    "    # -- Unpacking json\n",
    "    res_id = res[\"id\"]\n",
    "    img_id = res[\"img_id\"]\n",
    "    q = res[\"question\"]\n",
    "    sa = res[\"short_answer\"]\n",
    "    ra = res[\"reasoned_answer\"]\n",
    "    \n",
    "    # -- Img validation\n",
    "    IMG_PATH = os.path.join(IMG_DIR, f\"{img_id}.jpg\")\n",
    "    \n",
    "    if os.path.isfile(IMG_PATH):\n",
    "        raw_image = Image.open(IMG_PATH)\n",
    "    else:\n",
    "        print(f\"Invalid image {img_id} file\")\n",
    "        continue\n",
    "    \n",
    "    # -- Inference process\n",
    "    raw_factoid_eval_res = inference_llava(\n",
    "        model, processor,\n",
    "        FACTOID_EVAL_PROMPT.format(question = q, answer = sa),\n",
    "        raw_image\n",
    "    )\n",
    "    \n",
    "    Rs = [R0_PROMPT, R1_PROMPT, R2_PROMPT, R3_PROMPT, R4_PROMPT]\n",
    "    scores = []\n",
    "    for R in Rs:\n",
    "        r_eval_res = inference_llava(\n",
    "            model, processor,\n",
    "            R.format(question = q, answer = sa, reason = ra),\n",
    "            raw_image\n",
    "        )\n",
    "        scores.append(r_eval_res)\n",
    "    \n",
    "    # -- Log result\n",
    "    ids.append(res_id)\n",
    "#     if \"sample\" in img_id:\n",
    "#         subject_ids.append(subject_id)\n",
    "#         img_ids.append(img_id)\n",
    "#         times.append(time)\n",
    "    factoid_scores.append(raw_factoid_eval_res)\n",
    "    reasoning_scores.append(';'.join(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89fa767",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs,logics,clears,details,irrels,plauss=[],[],[],[],[],[]\n",
    "\n",
    "for ID, fs, rs in zip(ids,factoid_scores, reasoning_scores):\n",
    "    logic, clear, detail, irrel, plaus = (int(s) for s in rs.split(\";\"))\n",
    "    accs.append(int(fs))\n",
    "    logics.append(logic)\n",
    "    clears.append(clear)\n",
    "    details.append(detail)\n",
    "    irrels.append(irrel)\n",
    "    plauss.append(plaus)\n",
    "    \n",
    "\n",
    "# data = [\n",
    "#     {\n",
    "#         \"id\" : i,\n",
    "#         \"accuracy\": a,\n",
    "#         \"logic\" : l,\n",
    "#         \"clarity\" : c,\n",
    "#         \"detail\" : d,\n",
    "#         \"irrelevance\" : ir,\n",
    "#         \"plausibility\" : p\n",
    "#     }\n",
    "#     for i,a,l,c,d,ir,p in zip(ids,accs,logics,clears,details,irrels,plauss)\n",
    "# ]\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"id\" : i,\n",
    "        \"accuracy\": a,\n",
    "        \"logic\" : l,\n",
    "        \"clarity\" : c,\n",
    "        \"detail\" : d,\n",
    "        \"irrelevance\" : ir,\n",
    "        \"plausibility\" : p\n",
    "    }\n",
    "    for i,a,l,c,d,ir,p in zip(ids,accs,logics,clears,details,irrels,plauss)\n",
    "]\n",
    "\n",
    "with open(f\"sdg_eval.json\", 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Patrick (Mistral)",
   "language": "python",
   "name": "mistral_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
