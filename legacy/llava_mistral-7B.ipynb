{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30359c74",
   "metadata": {},
   "source": [
    "# LLaVa Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef39ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def load_config(config_path,config_name):\n",
    "    with open(os.path.join(config_path, config_name)) as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "config = load_config(\"../\",\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05aa8256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sample_001': 30, 'sample_008': 10, 'sample_007': 10, 'sample_005': 20, 'sample_009': 10, 'sample_002': 20}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "res = {}\n",
    "\n",
    "with open('sample_eval.json') as f:\n",
    "    d = json.load(f)\n",
    "\n",
    "for i in d:\n",
    "    if i['img_id'] not in res:\n",
    "        res[i['img_id']] = 1\n",
    "    else:\n",
    "        res[i['img_id']] += 1\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "add9cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9668e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants & Seed\n",
    "SEED = config[\"seed\"]\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Inputs\n",
    "IMG_PATH = config[\"image_path\"]\n",
    "IMG_ID = IMG_PATH.split('/')[-1].split('.')[0]\n",
    "\n",
    "# Prompt\n",
    "LR_PROMPT_TYPE = config['prompt']['lr']['filename']\n",
    "QG_PROMPT_TYPE = config[\"prompt\"][\"qg\"][\"filename\"]\n",
    "STR8_PROMPT_TYPE = config[\"prompt\"][\"str8\"][\"filename\"]\n",
    "STORY_PROMPT_TYPE = config[\"prompt\"][\"qgstory\"][\"story_filename\"]\n",
    "QGSTORY_PROMPT_TYPE = config[\"prompt\"][\"qgstory\"][\"qg_filename\"]\n",
    "\n",
    "LR_PROMPT_PATH = f\"{config['prompt']['lr']['parent']}{LR_PROMPT_TYPE}\"\n",
    "QG_PROMPT_PATH = f'{config[\"prompt\"][\"qg\"][\"parent\"]}{QG_PROMPT_TYPE}'\n",
    "STR8_PROMPT_PATH = f\"{config['prompt']['str8']['parent']}{STR8_PROMPT_TYPE}\"\n",
    "STORY_PROMPT_PATH = f\"{config['prompt']['qgstory']['parent']}{STORY_PROMPT_TYPE}\"\n",
    "QGSTORY_PROMPT_PATH = f\"{config['prompt']['qgstory']['parent']}{QGSTORY_PROMPT_TYPE}\"\n",
    "\n",
    "with open(LR_PROMPT_PATH, \"r\") as file:\n",
    "    LR_PROMPT= file.read()\n",
    "with open(QG_PROMPT_PATH,\"r\") as file:\n",
    "    QG_PROMPT = file.read()\n",
    "with open(STR8_PROMPT_PATH,\"r\") as file:\n",
    "    STR8_PROMPT = file.read()\n",
    "with open(STORY_PROMPT_PATH,\"r\") as file:\n",
    "    STORY_PROMPT = file.read()\n",
    "with open(QGSTORY_PROMPT_PATH,\"r\") as file:\n",
    "    QGSTORY_PROMPT = file.read()\n",
    "\n",
    "# Params\n",
    "MODEL_NAME = config[\"llava\"][\"model_name\"]\n",
    "MODEL_PATH = config[\"llava\"][\"model_path\"]\n",
    "PAIR_NUM = config[\"llava\"][\"model_params\"][\"pair_count\"]\n",
    "\n",
    "# Result\n",
    "LR_RESULT_PARENT_PATH = config[\"llava\"][\"result\"][\"lr_path\"].format(model_name = MODEL_NAME)\n",
    "QG_RESULT_PARENT_PATH = config[\"llava\"][\"result\"][\"qg_path\"].format(model_name = MODEL_NAME)\n",
    "JSON_RESULT_PARENT_PATH = config[\"llava\"][\"result\"][\"json_path\"].format(model_name = MODEL_NAME)\n",
    "STORY_RESULT_PARENT_PATH = config[\"llava\"][\"result\"][\"story_path\"].format(model_name = MODEL_NAME)\n",
    "QGSTORY_RESULT_PARENT_PATH = config[\"llava\"][\"result\"][\"qgstory_path\"].format(model_name = MODEL_NAME)\n",
    "STR8_RESULT_PARENT_PATH = config[\"llava\"][\"result\"][\"str8_path\"].format(model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d2802b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mistral_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  6.80it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import transformers\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_PATH, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    "#     load_in_4bit=True\n",
    ").to(0)\n",
    "processor = AutoProcessor.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54872dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_llava(model, processor, prompt, img, max_new_tokens=1500, do_sample=False, skip_special_tokens=True) -> str:\n",
    "    complete_prompt = f\"USER: <image>\\n{prompt}\\nASSISTANT:\"\n",
    "    \n",
    "    inputs = processor(\n",
    "        complete_prompt, \n",
    "        img, \n",
    "        return_tensors = 'pt'\n",
    "    ).to(0, torch.float16)\n",
    "    \n",
    "    raw_output = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens = max_new_tokens, \n",
    "        do_sample = do_sample\n",
    "    )\n",
    "    \n",
    "    output = processor.decode(raw_output[0], skip_special_tokens = skip_special_tokens)\n",
    "    output_trunc = output[output.index(\"ASSISTANT:\") + 11:]\n",
    "    \n",
    "    return output_trunc\n",
    "\n",
    "def exec_time(to, tt) -> str:\n",
    "    time_difference = tt - to\n",
    "\n",
    "    hours, remainder = divmod(time_difference.seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "    result_format = f\"{hours}h{minutes}m{seconds}s\"\n",
    "    \n",
    "    return result_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "967fa319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generate a 750 words description about the image. AVOID using any external assumptions or informations!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STORY_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e771f2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "raw_image = Image.open(IMG_PATH)\n",
    "\n",
    "# t_lrqg_o = datetime.now()\n",
    "# lr_out = inference_llava(\n",
    "#     model, processor, \n",
    "#     LR_PROMPT.format(number = PAIR_NUM), \n",
    "#     raw_image\n",
    "# )\n",
    "\n",
    "# qg_out = inference_llava(\n",
    "#     model, processor,\n",
    "#     QG_PROMPT.format(desc = lr_out, number = PAIR_NUM),\n",
    "#     raw_image\n",
    "# )\n",
    "# t_lrqg_t = datetime.now()\n",
    "\n",
    "\n",
    "\n",
    "# t_str8_o = datetime.now()\n",
    "# str8_out = inference_llava(\n",
    "#     model, processor,\n",
    "#     STR8_PROMPT.format(number = PAIR_NUM),\n",
    "#     raw_image\n",
    "# )\n",
    "# t_str8_t = datetime.now()\n",
    "\n",
    "\n",
    "\n",
    "t_qgstory_o = datetime.now()\n",
    "story_out = inference_llava(\n",
    "    model, processor,\n",
    "    STORY_PROMPT,\n",
    "    raw_image\n",
    ")\n",
    "\n",
    "qgstory_out = inference_llava(\n",
    "    model, processor,\n",
    "    QGSTORY_PROMPT.format(story = story_out, number = PAIR_NUM),\n",
    "    raw_image\n",
    ")\n",
    "t_qgstory_t = datetime.now()\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%m_%d_%Y-%H:%M:%S\")\n",
    "# lrqg_exec_time = exec_time(t_lrqg_o, t_lrqg_t)\n",
    "# qgstory_exec_time = exec_time(t_qgstory_o, t_qgstory_t)\n",
    "# str8_exec_time = exec_time(t_str8_t, t_str8_t)\n",
    "qgstory_exec_time = exec_time(t_qgstory_o, t_qgstory_t)\n",
    "\n",
    "\n",
    "\n",
    "FILENAME = f\"{IMG_ID}_{timestamp}.txt\" \n",
    "LR_RESULT_PATH = LR_RESULT_PARENT_PATH + FILENAME\n",
    "QG_RESULT_PATH = QG_RESULT_PARENT_PATH + FILENAME\n",
    "STORY_RESULT_PATH = STORY_RESULT_PARENT_PATH + FILENAME\n",
    "QGSTORY_RESULT_PATH = QGSTORY_RESULT_PARENT_PATH + FILENAME\n",
    "STR8_RESULT_PATH = STR8_RESULT_PARENT_PATH + FILENAME\n",
    "\n",
    "\n",
    "# with open(LR_RESULT_PATH,\"w\") as f:\n",
    "#     f.write(lr_out)\n",
    "# with open(QG_RESULT_PATH,\"w\") as f:\n",
    "#     f.write(qg_out)\n",
    "#     f.write(\"\\n\\n\")\n",
    "#     f.write(lrqg_exec_time)\n",
    "# with open(STORY_RESULT_PATH,\"w\") as f:\n",
    "#     f.write(story_out)\n",
    "#     f.write(\"\\n\\n\")\n",
    "#     f.write(lrqg_exec_time)\n",
    "# with open(QGSTORY_RESULT_PATH,\"w\") as f:\n",
    "#     f.write(qgstory_out)\n",
    "#     f.write(\"\\n\\n\")\n",
    "#     f.write(qgstory_exec_time)\n",
    "# with open(STR8_RESULT_PATH,\"w\") as f:\n",
    "#     f.write(str8_out)\n",
    "#     f.write(\"\\n\\n\")\n",
    "#     f.write(str8_exec_time)\n",
    "\n",
    "with open(\"sample_0012_qgstory.txt\",\"w\") as f:\n",
    "    f.write(qgstory_out)\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(qgstory_exec_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa781020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The image depicts a bustling city scene with a large group of people gathered around a food truck. The food truck is parked in the middle of the scene, and several people are standing around it, likely waiting to order or enjoying their meals. \\n\\nIn addition to the food truck, there are multiple traffic lights scattered throughout the scene, indicating that the area is well-regulated for vehicular and pedestrian traffic. A few cars can be seen in the background, further emphasizing the urban setting.\\n\\nThere are also a few bicycles parked or being ridden in the area, adding to the lively atmosphere. A backpack is visible on the ground, possibly belonging to one of the people in the scene. Overall, the image captures a vibrant city environment with people enjoying their time and engaging in various activities.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faaf09bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:25<00:25, 25.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH-0\n",
      "1. What is the main attraction in the image?\n",
      "S. Food truck\n",
      "L. The main attraction in the image is the food truck, which has drawn a large crowd of people who are standing around it, waiting to order or enjoying their meals.\n",
      "\n",
      "2. Where is the food truck parked?\n",
      "S. In the middle of the scene\n",
      "L. The food truck is parked in the middle of the scene, surrounded by a large group of people.\n",
      "\n",
      "3. Who are the people in the image?\n",
      "S. Pedestrians\n",
      "L. The people in the image are pedestrians, who are gathered around the food truck, likely waiting to order or enjoying their meals.\n",
      "\n",
      "4. How many traffic lights are visible in the scene?\n",
      "S. 5\n",
      "L. There are five traffic lights visible in the scene, indicating that the area is well-regulated for vehicular and pedestrian traffic.\n",
      "\n",
      "5. What are the people in the image doing?\n",
      "S. Standing around food truck\n",
      "L. The people in the image are standing around the food truck, likely waiting to order or enjoying their meals.\n",
      "\n",
      "6. What is the purpose of the traffic lights in the scene?\n",
      "S. Regulate traffic\n",
      "L. The purpose of the traffic lights in the scene is to regulate vehicular and pedestrian traffic, ensuring the safety and smooth flow of movement in the area.\n",
      "\n",
      "7. Who owns the backpack on the ground?\n",
      "S. Pedestrian\n",
      "L. The backpack on the ground likely belongs to one of the pedestrians in the scene, who may have placed it down while they order food from the food truck or enjoy their meal.\n",
      "\n",
      "8. What is the color of the food truck?\n",
      "S. White\n",
      "L. The food truck is white, which makes it stand out against the bustling city scene.\n",
      "\n",
      "9. How many bicycles are visible in the image?\n",
      "S. 2\n",
      "L. There are two bicycles visible in the image, which adds to the lively atmosphere of the city scene.\n",
      "\n",
      "10. What is the purpose of the food truck in the image?\n",
      "S. Serve food\n",
      "L. The purpose of the food truck in the image is to serve food to the pedestrians who have gathered around it, providing them with a convenient and enjoyable dining experience in the city.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:40<00:00, 20.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH-1\n",
      "1. What is the main attraction in the image?\n",
      "S. Food truck\n",
      "L. The main attraction in the image is the food truck, which has drawn a large crowd of people who are standing around it, waiting to order or enjoying their meals.\n",
      "\n",
      "2. Where is the food truck parked?\n",
      "S. In the middle of the scene\n",
      "L. The food truck is parked in the middle of the scene, surrounded by a large group of people.\n",
      "\n",
      "3. Who are the people in the image?\n",
      "S. Pedestrians\n",
      "L. The people in the image are pedestrians, who are gathered around the food truck, likely enjoying their time and engaging in various activities.\n",
      "\n",
      "4. How many traffic lights are visible in the scene?\n",
      "S. 5\n",
      "L. There are five traffic lights visible in the scene, indicating that the area is well-regulated for vehicular and pedestrian traffic.\n",
      "\n",
      "5. Is there any bicycle in the image?\n",
      "S. Yes\n",
      "L. Yes, there are a few bicycles parked or being ridden in the area, adding to the lively atmosphere.\n",
      "FINAL\n",
      "1. What is the main attraction in the image?\n",
      "S. Food truck\n",
      "L. The main attraction in the image is the food truck, which has drawn a large crowd of people who are standing around it, waiting to order or enjoying their meals.\n",
      "\n",
      "2. Where is the food truck parked?\n",
      "S. In the middle of the scene\n",
      "L. The food truck is parked in the middle of the scene, surrounded by a large group of people.\n",
      "\n",
      "3. Who are the people in the image?\n",
      "S. Pedestrians\n",
      "L. The people in the image are pedestrians, who are gathered around the food truck, likely waiting to order or enjoying their meals.\n",
      "\n",
      "4. How many traffic lights are visible in the scene?\n",
      "S. 5\n",
      "L. There are five traffic lights visible in the scene, indicating that the area is well-regulated for vehicular and pedestrian traffic.\n",
      "\n",
      "5. What are the people in the image doing?\n",
      "S. Standing around food truck\n",
      "L. The people in the image are standing around the food truck, likely waiting to order or enjoying their meals.\n",
      "\n",
      "6. What is the purpose of the traffic lights in the scene?\n",
      "S. Regulate traffic\n",
      "L. The purpose of the traffic lights in the scene is to regulate vehicular and pedestrian traffic, ensuring the safety and smooth flow of movement in the area.\n",
      "\n",
      "7. Who owns the backpack on the ground?\n",
      "S. Pedestrian\n",
      "L. The backpack on the ground likely belongs to one of the pedestrians in the scene, who may have placed it down while they order food from the food truck or enjoy their meal.\n",
      "\n",
      "8. What is the color of the food truck?\n",
      "S. White\n",
      "L. The food truck is white, which makes it stand out against the bustling city scene.\n",
      "\n",
      "9. How many bicycles are visible in the image?\n",
      "S. 2\n",
      "L. There are two bicycles visible in the image, which adds to the lively atmosphere of the city scene.\n",
      "\n",
      "10. What is the purpose of the food truck in the image?\n",
      "S. Serve food\n",
      "L. The purpose of the food truck in the image is to serve food to the pedestrians who have gathered around it, providing them with a convenient and enjoyable dining experience in the city.\n",
      "1. What is the main attraction in the image?\n",
      "S. Food truck\n",
      "L. The main attraction in the image is the food truck, which has drawn a large crowd of people who are standing around it, waiting to order or enjoying their meals.\n",
      "\n",
      "2. Where is the food truck parked?\n",
      "S. In the middle of the scene\n",
      "L. The food truck is parked in the middle of the scene, surrounded by a large group of people.\n",
      "\n",
      "3. Who are the people in the image?\n",
      "S. Pedestrians\n",
      "L. The people in the image are pedestrians, who are gathered around the food truck, likely enjoying their time and engaging in various activities.\n",
      "\n",
      "4. How many traffic lights are visible in the scene?\n",
      "S. 5\n",
      "L. There are five traffic lights visible in the scene, indicating that the area is well-regulated for vehicular and pedestrian traffic.\n",
      "\n",
      "5. Is there any bicycle in the image?\n",
      "S. Yes\n",
      "L. Yes, there are a few bicycles parked or being ridden in the area, adding to the lively atmosphere.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def batch_inference(image_path, model, processor, total_pair_count, pair_per_batch = 10):\n",
    "    raw_image = Image.open(image_path)\n",
    "\n",
    "    NUM_BATCH = ceil(total_pair_count / pair_per_batch)\n",
    "    LAST_BATCH = total_pair_count % pair_per_batch\n",
    "    \n",
    "    total_out = \"\"\n",
    "    \n",
    "    for batch in tqdm(range(NUM_BATCH)):\n",
    "        if batch == NUM_BATCH - 1:\n",
    "            pair_per_batch = LAST_BATCH\n",
    "\n",
    "        story_out = inference_llava(\n",
    "            model, processor,\n",
    "            STORY_PROMPT,\n",
    "            raw_image\n",
    "            \n",
    "        )\n",
    "\n",
    "        qgstory_out = inference_llava(\n",
    "            model, processor,\n",
    "            QGSTORY_PROMPT.format(story = story_out, number = pair_per_batch),\n",
    "            raw_image\n",
    "        )\n",
    "        \n",
    "        total_out += qgstory_out + \"\\n\"\n",
    "        \n",
    "        print(f\"BATCH-{batch}\")\n",
    "        print(qgstory_out)\n",
    "        \n",
    "        SEED = batch\n",
    "        set_seed(SEED)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%m_%d_%Y-%H:%M:%S\")\n",
    "    FILENAME = f\"{IMG_ID}_{timestamp}.txt\" \n",
    "    QGSTORY_RESULT_PATH = QGSTORY_RESULT_PARENT_PATH + FILENAME\n",
    "    SEED = config[\"seed\"]\n",
    "    \n",
    "    print(\"FINAL\")\n",
    "    print(total_out)\n",
    "    \n",
    "    with open(QGSTORY_RESULT_PATH,\"w\") as f:\n",
    "        f.write(total_out)\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "batch_inference(IMG_PATH, model, processor, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4449b1ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtotal_out\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_out' is not defined"
     ]
    }
   ],
   "source": [
    "total_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb30c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "raw_image = Image.open(IMG_PATH)\n",
    "\n",
    "\n",
    "t_lrqg_o = datetime.now()\n",
    "lr_out = inference_llava(\n",
    "    model, processor, \n",
    "    LR_PROMPT.format(number = PAIR_NUM), \n",
    "    raw_image\n",
    ")\n",
    "\n",
    "qg_out = inference_llava(\n",
    "    model, processor,\n",
    "    QG_PROMPT.format(desc = lr_out, number = PAIR_NUM),\n",
    "    raw_image\n",
    ")\n",
    "t_lrqg_t = datetime.now()\n",
    "\n",
    "\n",
    "\n",
    "t_str8_o = datetime.now()\n",
    "str8_out = inference_llava(\n",
    "    model, processor,\n",
    "    STR8_PROMPT.format(number = PAIR_NUM),\n",
    "    raw_image\n",
    ")\n",
    "t_str8_t = datetime.now()\n",
    "\n",
    "\n",
    "\n",
    "t_qgstory_o = datetime.now()\n",
    "story_out = inference_llava(\n",
    "    model, processor,\n",
    "    STORY_PROMPT,\n",
    "    raw_image\n",
    ")\n",
    "\n",
    "qgstory_out = inference_llava(\n",
    "    model, processor,\n",
    "    QGSTORY_PROMPT.format(story = story_out, number = PAIR_NUM),\n",
    "    raw_image\n",
    ")\n",
    "t_qgstory_t = datetime.now()\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%m_%d_%Y-%H:%M:%S\")\n",
    "lrqg_exec_time = exec_time(t_lrqg_o, t_lrqg_t)\n",
    "qgstory_exec_time = exec_time(t_qgstory_o, t_qgstory_t)\n",
    "str8_exec_time = exec_time(t_str8_t, t_str8_t)\n",
    "\n",
    "\n",
    "FILENAME = f\"{IMG_ID}_{timestamp}.txt\" \n",
    "LR_RESULT_PATH = LR_RESULT_PARENT_PATH + FILENAME\n",
    "QG_RESULT_PATH = QG_RESULT_PARENT_PATH + FILENAME\n",
    "STORY_RESULT_PATH = STORY_RESULT_PARENT_PATH + FILENAME\n",
    "QGSTORY_RESULT_PATH = QGSTORY_RESULT_PARENT_PATH + FILENAME\n",
    "STR8_RESULT_PATH = STR8_RESULT_PARENT_PATH + FILENAME\n",
    "\n",
    "\n",
    "with open(LR_RESULT_PATH,\"w\") as f:\n",
    "    f.write(lr_out)\n",
    "with open(QG_RESULT_PATH,\"w\") as f:\n",
    "    f.write(qg_out)\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(lrqg_exec_time)\n",
    "with open(STORY_RESULT_PATH,\"w\") as f:\n",
    "    f.write(story_out)\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(lrqg_exec_time)\n",
    "with open(QGSTORY_RESULT_PATH,\"w\") as f:\n",
    "    f.write(qgstory_out)\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(qgstory_exec_time)\n",
    "with open(STR8_RESULT_PATH,\"w\") as f:\n",
    "    f.write(str8_out)\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(str8_exec_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af4bea47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAIR_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dd13796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "total_data = []\n",
    "\n",
    "for num in [\"009\", \"007\", \"005\", \"002\", \"001\"]:\n",
    "    with open(f\"sample_{num}_qgstory.txt\",\"r\") as file:\n",
    "        # Read the entire file content\n",
    "        input_string = file.read()\n",
    "\n",
    "    # Define regular expression patterns\n",
    "    pattern_question = re.compile(r'\\d+\\.\\s(.+?)\\n')\n",
    "    pattern_short_answer = re.compile(r'S\\.\\s(.+?)\\n')\n",
    "    pattern_long_answer = re.compile(r'L\\.\\s(.+?)\\n')\n",
    "\n",
    "    # Find matches using regular expressions\n",
    "    questions = pattern_question.findall(input_string)\n",
    "    short_answers = pattern_short_answer.findall(input_string)\n",
    "    long_answers = pattern_long_answer.findall(input_string)\n",
    "\n",
    "\n",
    "    # Zip the results into a list of JSON objects\n",
    "    data = [\n",
    "        {\"id\":f\"sample_{num}\",\"question\": q, \"short_answer\": sa, \"reasoned_answer\": la}\n",
    "        for q, sa, la in zip(questions, short_answers, long_answers)\n",
    "    ]\n",
    "    \n",
    "    total_data += data\n",
    "\n",
    "print(len(total_data))\n",
    "with open(f\"sdg_out.json\", 'w') as json_file:\n",
    "    json.dump(total_data, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1db541c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Patrick (Mistral)",
   "language": "python",
   "name": "mistral_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
