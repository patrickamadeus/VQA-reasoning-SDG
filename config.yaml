### Model Parameters
seed: 42
llava:
    model_path: liuhaotian/llava-v1.5-7b
    params:
        pair_count: 10
lav2:
    model_path: ./model/LLaMa-7B/
    model_type: LORA-BIAS-7B
    params:
        pair_count: 10
bakllava:
    model_path: llava-hf/bakLlava-v1-hf
    params:
        pair_count: 10


### Prompting (updated)
prompt:
    lr:
        parent: ../prompt/LR/
        filename: base.txt
    qg:
        parent: ../prompt/QG/
        filename: 3W_split.txt
    eval:
        parent: ../../prompt/eval/
        factoid: factoid.txt
        reasoning: reasoning.txt
        complete: complete.txt

### Image
image_path: ../dataset/sample/sample_001.jpg

### Result
result:
    llava:
        lr_path: ../result/inference/LLaVa/LR/
        qg_path: ../result/inference/LLaVa/QG/
        json_path: ../result/inference/LLaVa/json/
    lav2:
        lr_path: ../result/inference/LLaMa-Adapter-V2/LR/
        qg_path: ../result/inference/LLaMa-Adapter-V2/QG/
        json_path: ../result/inference/LLaMa-Adapter-V2/json/
    bakllava:
        lr_path: ../result/inference/BakLLaVa/LR/
        qg_path: ../result//inference/BakLLaVa/QG/
        json_path: ../result/inference/BakLLaVa/json/
        
        
### Eval
eval:
    factoid_prompt_path: ../eval/
    sample:
        json_parent: ./json/sample/
        json_filename: sample_24011309.json
    llava:
        model: openai/clip-vit-base-patch32
        json_path: ../result/LLaVa/json/
    